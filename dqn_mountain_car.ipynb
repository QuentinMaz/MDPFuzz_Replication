{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "## Step-by-step Instructions\n",
    "\n",
    "Import the required classes and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from mdpfuzz.executor import Executor\n",
    "from mdpfuzz.logger import FuzzerLogger\n",
    "from mdpfuzz.mdpfuzz import Fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the DQN agent\n",
    "\n",
    "Define the path for saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"dqn_mountain_car.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a Callback for stopping training after a specific failure rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnFailureRateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for stopping training once the agent has reached a specific failure rate threshold.\n",
    "\n",
    "    Args:\n",
    "        eval_env (gym.Env): The environment used for evaluation.\n",
    "        eval_frequency (int): Frequency of the callback.\n",
    "        num_eval_episodes (int, optional): The number of episodes to approximate the failure rate of the agent. Default to 100.\n",
    "        failure_rate_treshold (float, optional): Failure rate threshold to stop training. Default to 0.10.\n",
    "        start_on_steps (int, optional): Steps after which the callback will be called. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        eval_frequency: int,\n",
    "        num_eval_episodes=int,\n",
    "        failure_rate_treshold=0.10,\n",
    "        start_on_steps=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "        self.failure_rate_treshold = failure_rate_treshold\n",
    "        self.start_on_steps = start_on_steps\n",
    "\n",
    "    def _on_step(self):\n",
    "\n",
    "        if self.n_calls >= self.start_on_steps and (\n",
    "            self.n_calls % self.eval_frequency == 0\n",
    "        ):\n",
    "            failure_rate = self._evaluate_failure_rate()\n",
    "            if self.verbose > 0:\n",
    "                print(\n",
    "                    \"Failure rate at steps {}: {:.2%}.\".format(\n",
    "                        self.n_calls, failure_rate\n",
    "                    )\n",
    "                )\n",
    "            if failure_rate <= self.failure_rate_treshold:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _evaluate_failure_rate(self) -> float:\n",
    "        num_failures = 0\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            obs, info = self.eval_env.reset()\n",
    "            terminated = truncated = False\n",
    "            state = False\n",
    "            while not (terminated or truncated):\n",
    "                action, state = self.model.predict(obs, state=state, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
    "            num_failures += int(terminated == False)\n",
    "        failure_rate = num_failures / self.num_eval_episodes\n",
    "        return failure_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, create the callback, the agent and trains the latter until it reaches a failure rate lower than 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a DQN policy for Mountain Car.\n",
    "Hyperparameters are borrowed from:\n",
    "https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/dqn_sb3.ipynb\n",
    "\"\"\"\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "dqn_model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    train_freq=16,\n",
    "    gradient_steps=8,\n",
    "    gamma=0.99,\n",
    "    exploration_fraction=0.2,\n",
    "    exploration_final_eps=0.07,\n",
    "    target_update_interval=600,\n",
    "    learning_starts=1000,\n",
    "    buffer_size=10000,\n",
    "    batch_size=128,\n",
    "    learning_rate=4e-3,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=2,\n",
    ")\n",
    "\n",
    "callback = StopOnFailureRateCallback(\n",
    "    eval_env=gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"),\n",
    "    eval_frequency=500,\n",
    "    num_eval_episodes=100,\n",
    "    failure_rate_treshold=0.10,\n",
    "    verbose=1,\n",
    "    start_on_steps=82_000,\n",
    ")\n",
    "\n",
    "dqn_model.learn(total_timesteps=90_000, callback=callback)\n",
    "print(\"Training stops after {} steps.\".format(callback.n_calls))\n",
    "dqn_model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the agent\n",
    "\n",
    "The following defines an *Executor* class for the *Mountain Car* environment.\n",
    "The inputs are the inital x position of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarExecutor(Executor):\n",
    "\n",
    "    def __init__(self, sim_steps, env_seed) -> None:\n",
    "        super().__init__(sim_steps, env_seed)\n",
    "        self.env = gym.make(\n",
    "            \"MountainCar-v0\", render_mode=\"rgb_array\"\n",
    "        )  # type: TimeLimit\n",
    "        # from MountainCar-v0 specifications:\n",
    "        # lower and upper bounds of the initial x position of the car\n",
    "        self.low = -0.6\n",
    "        self.high = -0.4\n",
    "        self.mutation_intensity = 0.05\n",
    "\n",
    "    def generate_input(self, rng: np.random.Generator) -> np.ndarray:\n",
    "        # from the original implementation\n",
    "        return np.array(\n",
    "            [rng.uniform(low=self.low, high=self.high), 0.0], dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def generate_inputs(self, rng: np.random.Generator, n: int) -> np.ndarray:\n",
    "        if n == 1:\n",
    "            return self.generate_input(rng)\n",
    "        else:\n",
    "            return np.vstack(\n",
    "                [self.generate_input(rng) for _ in range(n)], dtype=np.float32\n",
    "            )\n",
    "\n",
    "    def mutate(\n",
    "        self, input: np.ndarray, rng: np.random.Generator, **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        position, velocity = input\n",
    "        new_position = np.clip(\n",
    "            rng.normal(position, self.mutation_intensity), self.low, self.high\n",
    "        )\n",
    "        return np.array([new_position, velocity], dtype=np.float32)\n",
    "\n",
    "    def load_policy(self, **kwargs):\n",
    "        model_path = kwargs[\"model_path\"]\n",
    "        return DQN.load(model_path)\n",
    "\n",
    "    def execute_policy(\n",
    "        self, input: np.ndarray, policy: DQN\n",
    "    ) -> Tuple[float, bool, np.ndarray, float]:\n",
    "        t0 = time.time()\n",
    "        obs, _info = self.env.reset(seed=self.env_seed)\n",
    "\n",
    "        # sets the position and velocity of the car w.r.t `input`\n",
    "        position, velocity = input\n",
    "        self.env.unwrapped.state = (position, velocity)\n",
    "        obs = np.array(self.env.unwrapped.state, dtype=np.float32)\n",
    "\n",
    "        acc_reward = 0.0\n",
    "        state = None\n",
    "        obs_seq = []\n",
    "        while True:\n",
    "            obs_seq.append(obs)\n",
    "            action, state = policy.predict(obs, state=state, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _info = self.env.step(action)\n",
    "            acc_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return (\n",
    "            acc_reward,\n",
    "            not terminated,\n",
    "            np.array(obs_seq, dtype=np.float32),\n",
    "            time.time() - t0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an executor and load the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = MountainCarExecutor(sim_steps=200, env_seed=0)\n",
    "policy = executor.load_policy(model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a test budget of 2500 test cases and run Random Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_budget = 2500\n",
    "fuzzer = Fuzzer(random_seed=0, k=4, tau=0.1, gamma=0.01, executor=executor)\n",
    "fuzzer.random_testing(\n",
    "    n=testing_budget,\n",
    "    policy=policy,\n",
    "    path=\"random_testing\",\n",
    "    local_sensitivity=True,\n",
    "    exp_name=\"Mountain Car\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test the agent with MDPFuzz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fuzzer\n",
    "fuzzer = Fuzzer(random_seed=0, k=4, tau=0.1, gamma=0.01, executor=executor)\n",
    "fuzzer.fuzzing(\n",
    "    n=500,\n",
    "    test_budget=testing_budget,\n",
    "    policy=policy,\n",
    "    saving_path=\"mdpfuzz\",\n",
    "    local_sensitivity=True,\n",
    "    exp_name=\"Mountain Car\",\n",
    "    save_logs_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the results\n",
    "\n",
    "We can use the logs to count the number of failures revealed by the two methods and plot the evolution over the test cases.\n",
    "Define a simple function to count the failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_failures(failures: np.ndarray) -> np.ndarray:\n",
    "    if failures.dtype != int:\n",
    "        failures = failures.astype(int)\n",
    "\n",
    "    num_failures = 0\n",
    "    acc_failures = []\n",
    "    for f in failures:\n",
    "        num_failures += f\n",
    "        acc_failures.append(num_failures)\n",
    "    return np.array(acc_failures, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, extract the failures from the logs, count the failures and plot the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of failures found by Random Testing\n",
    "rt_failures = accumulate_failures(\n",
    "    FuzzerLogger(\"random_testing_logs.txt\")\n",
    "    .load_logs()[\"oracle\"]\n",
    "    .astype(int)\n",
    "    .to_numpy()\n",
    ")\n",
    "# number of failures found by MDPFuzz\n",
    "mdpfuzz_failures = accumulate_failures(\n",
    "    FuzzerLogger(\"mdpfuzz_logs.txt\").load_logs()[\"oracle\"].astype(int).to_numpy()\n",
    ")\n",
    "\n",
    "# code for comparing the results\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "x = np.arange(len(rt_failures))\n",
    "ax.plot(x, rt_failures, label=\"Random Testing\")\n",
    "ax.plot(x, mdpfuzz_failures, label=\"MDPFuzz\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"# Iterations\")\n",
    "ax.set_ylabel(\"# Failures\")\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_title(\"Number of failures over test iterations\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"failure_results_comparison_demo.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
